Step 1 - Create Narrow Table

Header:
-------------------

CREATE  TABLE IF NOT EXISTS pr_ccpcogxph_gbd_r000_sg.clm_wgs_gncclmp_cogx_TEST (
ddc_cd_dcn varchar(11) COMMENT '', 
ddc_cd_dcn_cc varchar(2) COMMENT '', 
ddc_cd_itm_cde varchar(2) COMMENT '', 
ddc_cd_clm_compl_dte decimal(9,0) COMMENT '', 
ddc_cd_clm_pay_act_1 varchar(1) COMMENT '', 
ddc_cd_clm_pay_act_2_6 varchar(5) COMMENT '', 
ddc_cd_cert_nbr1 varchar(3) COMMENT '', 
ddc_cd_cert_nbr2 varchar(2) COMMENT '', 
ddc_cd_cert_nbr3 varchar(4) COMMENT '', 
ddc_cd_pat_mbr_cde varchar(2) COMMENT '', 
ddc_cd_grp_nbr varchar(10) COMMENT '', 
ddc_cd_svc_from_dte decimal(9,0) COMMENT '', 
ddc_cd_svc_thru_dte decimal(9,0) COMMENT '',
ddc_cd_prvdr_tax_id varchar(9) COMMENT '', 
ddc_cd_prvdr_nme varchar(25) COMMENT '', 
ddc_cd_prvdr_sec_nme varchar(25) COMMENT '', 
ddc_cd_prvdr_spclty_cde varchar(3) COMMENT '', 
ddc_cd_prvdr_lic_alpha varchar(3) COMMENT '', 
ddc_cd_prvdr_lic_nmrc varchar(8) COMMENT '', 
ddc_cd_tot_chrg_amt decimal(11,2) COMMENT '', 
ddc_cd_med_rec_nbr_2 varchar(10) COMMENT '', 
ddc_cd_med_rec_nbr varchar(10) COMMENT '', 
ddc_cd_icda_cde_1 varchar(10) COMMENT '', 
ddc_cd_icda_cde_2 varchar(10) COMMENT '', 
ddc_cd_icda_cde_3 varchar(10) COMMENT '', 
ddc_cd_icda_cde_4 varchar(10) COMMENT '', 
ddc_cd_icda_cde_5 varchar(10) COMMENT '', 
ddc_cd_its_home_ind varchar(1) COMMENT '', 
ddc_cd_its_orig_sccf_nbr_new varchar(17) COMMENT '', 
ddc_cd_its_host_prvdr_ind varchar(1) COMMENT '', 
ddc_cd_prvdr_ind varchar(1) COMMENT '', 
ddc_cd_clm_type varchar(2) COMMENT '', 
load_dtm varchar(50) COMMENT '' 
)
PARTITIONED BY ( 
load_ingstn_id varchar(8))
stored as parquet
-------------------

Detail:
----------------------------------

CREATE  TABLE IF NOT EXISTS pr_ccpcogxph_gbd_r000_sg.clm_wgs_gncdtlp_cogx_TEST (
gnchiios_hclm_dcn varchar(11) COMMENT '', 
gnchiios_hclm_item_cde varchar(2) COMMENT '', 
ddc_dtl_lne_nbr varchar(2) COMMENT '', 
ddc_dtl_icda_pntr_1 varchar(1) COMMENT '', 
ddc_dtl_prcdr_cde varchar(5) COMMENT '', 
ddc_dtl_svc_cde_1_3 varchar(3) COMMENT '', 
ddc_dtl_proc_svc_cls_1 varchar(3) COMMENT '', 
ddc_dtl_proc_svc_cls_2 varchar(3) COMMENT '', 
ddc_dtl_proc_svc_cls_3 varchar(3) COMMENT '', 
ddc_dtl_pcodec_hcpcs_cde varchar(5) COMMENT '', 
ddc_dtl_blld_amt decimal(11,2) COMMENT '', 
ddc_dtl_unts_occur decimal(3,0) COMMENT '', 
ddc_dtl_units_occur decimal(11,2) COMMENT '', 
ddc_dtl_prcdr_modfr_cde varchar(2) COMMENT '', 
ddc_dtl_pcodec_hcpcs_mod varchar(2) COMMENT '', 
ddc_dtl_mod_cde_1 varchar(2) COMMENT '', 
ddc_dtl_mod_cde_2 varchar(2) COMMENT '', 
ddc_dtl_mod_cde_3 varchar(2) COMMENT '', 
ddc_dtl_hcfa_pt_cde varchar(2) COMMENT '', 
ddc_dtl_pt_cde varchar(1) COMMENT '', 
ddc_dtl_elig_expsn_amt decimal(11,2) COMMENT '', 
ddc_dtl_svc_from_dte decimal(9,0) COMMENT '', 
ddc_dtl_svc_thru_dte decimal(9,0) COMMENT '' 
   
)
PARTITIONED BY ( 
load_ingstn_id varchar(8))
stored as parquet
   
----------------------------------

EA2:
------------------------------------
CREATE  TABLE IF NOT EXISTS pr_ccpcogxph_gbd_r000_sg.clm_wgs_gncnatp_ea2_cogx_TEST (
ddc_nat_ea2_type_of_bill varchar(1) COMMENT '',
gnchiios_hclm_dcn varchar(11) COMMENT '', 
gnchiios_hclm_dcn_cc varchar(2) COMMENT '', 
gnchiios_hclm_item_cde varchar(2) COMMENT ''
   
)
PARTITIONED BY ( 
load_ingstn_id varchar(8))
stored as parquet
 
------------------------------------

Step 2 - Update application_prd.properties
(Note: make sure to use the latest production properties)
-------------------------------------------
### Update the following attribute
inbound-hive-db=pr_ccpcogxph_gbd_r000_sg
stage-hive-db=pr_ccpcogxph_gbd_r000_sg

--------------------------------------------

Step 3 -  Create query_cogxHiveBDFSyncSITAll.properties
(Get the latest from Bitbucket)
-------------------------------------------------


hbase_table_name=cogx_claims
hbase_table_columnfamily=c1
hbase_table_columnname=jsonData

### On Linux need Double Quote
hbaseconfigfile="hbase-site.xml"

## Audit Table and Hbase target
teradata_table_name=cogx_claims
ABCSaveFormat=hive
ABCHIVEDBTABLE=<<auditSchema>>.cogx_etl_audit
cogx_audit_query="select max(CAST(date_format(to_date(lastupdate),'yyyyMMdd') as INT)) from <<auditSchema>>.cogx_um_teradata_hb_audit where program='<<programName>>' and status ='completed' "


## Header query
query_clm_wgs_gncclmp=" SELECT distinct  hdr.ddc_cd_dcn as ddc_cd_dcn,    hdr.ddc_cd_dcn_cc as ddc_cd_dcn_cc,     hdr.ddc_cd_itm_cde as ddc_cd_itm_cde,     hdr.ddc_cd_clm_compl_dte as ddc_cd_clm_compl_dte,     hdr.ddc_cd_clm_pay_act_1 as ddc_cd_clm_pay_act_1,     hdr.ddc_cd_clm_pay_act_2_6 as ddc_cd_clm_pay_act_2_6,     hdr.ddc_cd_cert_nbr1 as ddc_cd_cert_nbr1,     hdr.ddc_cd_cert_nbr2 as ddc_cd_cert_nbr2,     hdr.ddc_cd_cert_nbr3 as ddc_cd_cert_nbr3,     hdr.ddc_cd_pat_mbr_cde as ddc_cd_pat_mbr_cde,     hdr.ddc_cd_grp_nbr as ddc_cd_grp_nbr,     hdr.ddc_cd_svc_from_dte as ddc_cd_svc_from_dte,     hdr.ddc_cd_svc_thru_dte as ddc_cd_svc_thru_dte,     hdr.ddc_cd_prvdr_tax_id as ddc_cd_prvdr_tax_id,     hdr.ddc_cd_prvdr_nme as ddc_cd_prvdr_nme,     hdr.ddc_cd_prvdr_sec_nme as ddc_cd_prvdr_sec_nme,     hdr.ddc_cd_prvdr_spclty_cde as ddc_cd_prvdr_spclty_cde,     hdr.ddc_cd_prvdr_lic_alpha as ddc_cd_prvdr_lic_alpha,     hdr.ddc_cd_prvdr_lic_nmrc as ddc_cd_prvdr_lic_nmrc,     hdr.ddc_cd_tot_chrg_amt as ddc_cd_tot_chrg_amt,     hdr.ddc_cd_med_rec_nbr_2 as ddc_cd_med_rec_nbr_2,     hdr.ddc_cd_med_rec_nbr as ddc_cd_med_rec_nbr,     hdr.ddc_cd_icda_cde_1 as ddc_cd_icda_cde_1,     hdr.ddc_cd_icda_cde_2 as ddc_cd_icda_cde_2,     hdr.ddc_cd_icda_cde_3 as ddc_cd_icda_cde_3,     hdr.ddc_cd_icda_cde_4 as ddc_cd_icda_cde_4,     hdr.ddc_cd_icda_cde_5 as ddc_cd_icda_cde_5,     hdr.ddc_cd_its_home_ind as ddc_cd_its_home_ind,     hdr.ddc_cd_its_orig_sccf_nbr_new as ddc_cd_its_orig_sccf_nbr_new,     hdr.ddc_cd_its_host_prvdr_ind as ddc_cd_its_host_prvdr_ind,     hdr.ddc_cd_prvdr_ind as ddc_cd_prvdr_ind,     hdr.ddc_cd_clm_type as ddc_cd_clm_type,  hdr.src_load_dtm as load_dtm     , hdr.load_ingstn_id as load_ingstn_id           FROM <<sourceDB>>.clm_wgs_gncclmp hdr   WHERE hdr.ddc_cd_itm_cde = '80'   AND hdr.ddc_cd_clm_compl_dte >= <<histDate>>  <<Header_Additional_Filter>>  "
Header_Additional_Filter="  "
#Header_Additional_Filter=" and hdr.load_ingstn_id = 20180801 "
#Header_Additional_Filter="   and hdr.ddc_cd_svc_from_dte = 20180814 and hdr.ddc_cd_svc_thru_dte = 20180814 AND hdr.ddc_cd_clm_compl_dte >= 20160607 and hdr.ddc_cd_pat_mbr_cde = 20 "

## Detail query
query_clm_wgs_gncdtlp="select  distinct  dtl.gnchiios_hclm_dcn as gnchiios_hclm_dcn,     dtl.gnchiios_hclm_item_cde as gnchiios_hclm_item_cde,     dtl.ddc_dtl_lne_nbr as ddc_dtl_lne_nbr,     dtl.ddc_dtl_icda_pntr_1 as ddc_dtl_icda_pntr_1,     dtl.ddc_dtl_prcdr_cde as ddc_dtl_prcdr_cde,     dtl.ddc_dtl_svc_cde_1_3 as ddc_dtl_svc_cde_1_3,     dtl.ddc_dtl_proc_svc_cls_1 as ddc_dtl_proc_svc_cls_1,     dtl.ddc_dtl_proc_svc_cls_2 as ddc_dtl_proc_svc_cls_2,     dtl.ddc_dtl_proc_svc_cls_3 as ddc_dtl_proc_svc_cls_3,     dtl.ddc_dtl_pcodec_hcpcs_cde as ddc_dtl_pcodec_hcpcs_cde,     dtl.ddc_dtl_blld_amt as ddc_dtl_blld_amt,     dtl.ddc_dtl_unts_occur as ddc_dtl_unts_occur,     dtl.ddc_dtl_units_occur as ddc_dtl_units_occur,     dtl.ddc_dtl_prcdr_modfr_cde as ddc_dtl_prcdr_modfr_cde,     dtl.ddc_dtl_pcodec_hcpcs_mod as ddc_dtl_pcodec_hcpcs_mod,     dtl.ddc_dtl_mod_cde_1 as ddc_dtl_mod_cde_1,     dtl.ddc_dtl_mod_cde_2 as ddc_dtl_mod_cde_2,     dtl.ddc_dtl_mod_cde_3 as ddc_dtl_mod_cde_3,     dtl.ddc_dtl_hcfa_pt_cde as ddc_dtl_hcfa_pt_cde,     dtl.ddc_dtl_pt_cde as ddc_dtl_pt_cde,     dtl.ddc_dtl_elig_expsn_amt as ddc_dtl_elig_expsn_amt,     dtl.ddc_dtl_svc_from_dte AS ddc_dtl_svc_from_dte,     dtl.ddc_dtl_svc_thru_dte AS ddc_dtl_svc_thru_dte  ,      dtl.load_ingstn_id as load_ingstn_id     from <<sourceDB>>.clm_wgs_gncdtlp dtl where dtl.gnchiios_hclm_item_cde='80'  <<Detail_Additional_Filter>> " 
Detail_Additional_Filter=" "
#Detail_Additional_Filter=" and dtl.load_ingstn_id = 20180801 "


## EA2 query
query_clm_wgs_gncnatp_ea2=" SELECT  distinct EA2.ddc_nat_ea2_type_of_bill as ddc_nat_ea2_type_of_bill,   EA2.gnchiios_hclm_dcn as gnchiios_hclm_dcn,   EA2.gnchiios_hclm_dcn_cc as gnchiios_hclm_dcn_cc,   EA2.gnchiios_hclm_item_cde as   gnchiios_hclm_item_cde,   EA2.load_ingstn_id as load_ingstn_id   FROM   <<sourceDB>>.clm_wgs_gncnatp_ea2 EA2   WHERE EA2.gnchiios_hclm_item_cde = '80' <<ea_Additional_Filter>> "
#ea_Additional_Filter=" and EA2.load_ingstn_id = 20180801  "
ea_Additional_Filter="   "

NarrowHeaderDB="<<sourceDB>>.clm_wgs_gncclmp_cogx_TEST"
NarrowDetailDB="<<sourceDB>>.clm_wgs_gncdtlp_cogx_TEST"
NarrowEA2DB="<<sourceDB>>.clm_wgs_gncnatp_ea2_cogx_TEST"

## roll back period
backout_months=36

# Incremental loading Config
default_incremental_startdt=20190401
isIncremental=No
force_default_incremental=no

# msck repair
query_msck_header="msck repair table <<sourceDB>>.clm_wgs_gncclmp"
query_msck_detail="msck repair table <<sourceDB>>.clm_wgs_gncdtlp"
query_msck_ea2="msck repair table <<sourceDB>>.clm_wgs_gncnatp_ea2"



-------------------------------------------------

Step 4 - create query_cogxHiveBDFSyncSITIncremental.properties
(Get the latest from Bitbucket)
-----------------------------------


hbase_table_name=cogx_claims
hbase_table_columnfamily=c1
hbase_table_columnname=jsonData

### On Linux need Double Quote
hbaseconfigfile="hbase-site.xml"

## Audit Table and Hbase target
teradata_table_name=cogx_claims
ABCSaveFormat=hive
ABCHIVEDBTABLE=<<auditSchema>>.cogx_etl_audit
cogx_audit_query="select max(CAST(date_format(to_date(lastupdate),'yyyyMMdd') as INT)) from <<auditSchema>>.cogx_um_teradata_hb_audit where program='<<programName>>' and status ='completed' "



## Header query
query_clm_wgs_gncclmp=" SELECT  distinct  hdr.ddc_cd_dcn as ddc_cd_dcn,    hdr.ddc_cd_dcn_cc as ddc_cd_dcn_cc,     hdr.ddc_cd_itm_cde as ddc_cd_itm_cde,     hdr.ddc_cd_clm_compl_dte as ddc_cd_clm_compl_dte,     hdr.ddc_cd_clm_pay_act_1 as ddc_cd_clm_pay_act_1,     hdr.ddc_cd_clm_pay_act_2_6 as ddc_cd_clm_pay_act_2_6,     hdr.ddc_cd_cert_nbr1 as ddc_cd_cert_nbr1,     hdr.ddc_cd_cert_nbr2 as ddc_cd_cert_nbr2,     hdr.ddc_cd_cert_nbr3 as ddc_cd_cert_nbr3,     hdr.ddc_cd_pat_mbr_cde as ddc_cd_pat_mbr_cde,     hdr.ddc_cd_grp_nbr as ddc_cd_grp_nbr,     hdr.ddc_cd_svc_from_dte as ddc_cd_svc_from_dte,     hdr.ddc_cd_svc_thru_dte as ddc_cd_svc_thru_dte,     hdr.ddc_cd_prvdr_tax_id as ddc_cd_prvdr_tax_id,     hdr.ddc_cd_prvdr_nme as ddc_cd_prvdr_nme,     hdr.ddc_cd_prvdr_sec_nme as ddc_cd_prvdr_sec_nme,     hdr.ddc_cd_prvdr_spclty_cde as ddc_cd_prvdr_spclty_cde,     hdr.ddc_cd_prvdr_lic_alpha as ddc_cd_prvdr_lic_alpha,     hdr.ddc_cd_prvdr_lic_nmrc as ddc_cd_prvdr_lic_nmrc,     hdr.ddc_cd_tot_chrg_amt as ddc_cd_tot_chrg_amt,     hdr.ddc_cd_med_rec_nbr_2 as ddc_cd_med_rec_nbr_2,     hdr.ddc_cd_med_rec_nbr as ddc_cd_med_rec_nbr,     hdr.ddc_cd_icda_cde_1 as ddc_cd_icda_cde_1,     hdr.ddc_cd_icda_cde_2 as ddc_cd_icda_cde_2,     hdr.ddc_cd_icda_cde_3 as ddc_cd_icda_cde_3,     hdr.ddc_cd_icda_cde_4 as ddc_cd_icda_cde_4,     hdr.ddc_cd_icda_cde_5 as ddc_cd_icda_cde_5,     hdr.ddc_cd_its_home_ind as ddc_cd_its_home_ind,     hdr.ddc_cd_its_orig_sccf_nbr_new as ddc_cd_its_orig_sccf_nbr_new,     hdr.ddc_cd_its_host_prvdr_ind as ddc_cd_its_host_prvdr_ind,     hdr.ddc_cd_prvdr_ind as ddc_cd_prvdr_ind,     hdr.ddc_cd_clm_type as ddc_cd_clm_type,  hdr.src_load_dtm as load_dtm     , hdr.load_ingstn_id as load_ingstn_id           FROM <<sourceDB>>.clm_wgs_gncclmp hdr   WHERE hdr.ddc_cd_itm_cde = '80'   and hdr.load_ingstn_id >= <<load_date>>  AND hdr.ddc_cd_clm_compl_dte >= <<histDate>>  <<Header_Additional_Filter>>  "
Header_Additional_Filter="  "
#Header_Additional_Filter=" and hdr.load_ingstn_id = 20180801 "


#Header_Additional_Filter="   and hdr.ddc_cd_svc_from_dte = 20180814 and hdr.ddc_cd_svc_thru_dte = 20180814 AND hdr.ddc_cd_clm_compl_dte >= 20160607 and hdr.ddc_cd_pat_mbr_cde = 20 "

## Detail query
query_clm_wgs_gncdtlp="select  distinct  dtl.gnchiios_hclm_dcn as gnchiios_hclm_dcn,     dtl.gnchiios_hclm_item_cde as gnchiios_hclm_item_cde,     dtl.ddc_dtl_lne_nbr as ddc_dtl_lne_nbr,     dtl.ddc_dtl_icda_pntr_1 as ddc_dtl_icda_pntr_1,     dtl.ddc_dtl_prcdr_cde as ddc_dtl_prcdr_cde,     dtl.ddc_dtl_svc_cde_1_3 as ddc_dtl_svc_cde_1_3,     dtl.ddc_dtl_proc_svc_cls_1 as ddc_dtl_proc_svc_cls_1,     dtl.ddc_dtl_proc_svc_cls_2 as ddc_dtl_proc_svc_cls_2,     dtl.ddc_dtl_proc_svc_cls_3 as ddc_dtl_proc_svc_cls_3,     dtl.ddc_dtl_pcodec_hcpcs_cde as ddc_dtl_pcodec_hcpcs_cde,     dtl.ddc_dtl_blld_amt as ddc_dtl_blld_amt,     dtl.ddc_dtl_unts_occur as ddc_dtl_unts_occur,     dtl.ddc_dtl_units_occur as ddc_dtl_units_occur,     dtl.ddc_dtl_prcdr_modfr_cde as ddc_dtl_prcdr_modfr_cde,     dtl.ddc_dtl_pcodec_hcpcs_mod as ddc_dtl_pcodec_hcpcs_mod,     dtl.ddc_dtl_mod_cde_1 as ddc_dtl_mod_cde_1,     dtl.ddc_dtl_mod_cde_2 as ddc_dtl_mod_cde_2,     dtl.ddc_dtl_mod_cde_3 as ddc_dtl_mod_cde_3,     dtl.ddc_dtl_hcfa_pt_cde as ddc_dtl_hcfa_pt_cde,     dtl.ddc_dtl_pt_cde as ddc_dtl_pt_cde,     dtl.ddc_dtl_elig_expsn_amt as ddc_dtl_elig_expsn_amt,     dtl.ddc_dtl_svc_from_dte AS ddc_dtl_svc_from_dte,     dtl.ddc_dtl_svc_thru_dte AS ddc_dtl_svc_thru_dte  ,      dtl.load_ingstn_id as load_ingstn_id     from <<sourceDB>>.clm_wgs_gncdtlp dtl where dtl.gnchiios_hclm_item_cde='80'  and dtl.load_ingstn_id >= <<load_date>>   <<Detail_Additional_Filter>> " 
Detail_Additional_Filter=" "
#Detail_Additional_Filter=" and dtl.load_ingstn_id = 20180801 "




## EA2 query
query_clm_wgs_gncnatp_ea2=" SELECT  distinct EA2.ddc_nat_ea2_type_of_bill as ddc_nat_ea2_type_of_bill,   EA2.gnchiios_hclm_dcn as gnchiios_hclm_dcn,   EA2.gnchiios_hclm_dcn_cc as gnchiios_hclm_dcn_cc,   EA2.gnchiios_hclm_item_cde as   gnchiios_hclm_item_cde,   EA2.load_ingstn_id as load_ingstn_id   FROM   <<sourceDB>>.clm_wgs_gncnatp_ea2 EA2   WHERE EA2.gnchiios_hclm_item_cde = '80'  and EA2.load_ingstn_id >= <<load_date>>   <<ea_Additional_Filter>> "
#ea_Additional_Filter=" and EA2.load_ingstn_id = 20180801  "
ea_Additional_Filter="   "




NarrowHeaderDB="<<sourceDB>>.clm_wgs_gncclmp_cogx_TEST"
NarrowDetailDB="<<sourceDB>>.clm_wgs_gncdtlp_cogx_TEST"
NarrowEA2DB="<<sourceDB>>.clm_wgs_gncnatp_ea2_cogx_TEST"

## roll back period
backout_months=36

# Incremental loading Config
default_incremental_startdt=20190521
isIncremental=yes
force_default_incremental=yes

# msck repair
query_msck_header="msck repair table <<sourceDB>>.clm_wgs_gncclmp"
query_msck_detail="msck repair table <<sourceDB>>.clm_wgs_gncdtlp"
query_msck_ea2="msck repair table <<sourceDB>>.clm_wgs_gncnatp_ea2"



--------------------------------------

Step 5 - upload all the configuration files and new jars
--------------------------------------------
(Use the correct jar name)

hadoop fs -rm /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/ds-cogx-etl-2.0.7.jar
hadoop fs -put  /pr/app/ve2/ccp/cogx/phi/gbd/r000/bin/ds-cogx-etl-2.0.7.jar   /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/


hadoop fs -rm /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/application_prd.properties
hadoop fs -rm /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFSyncSITAll.properties
hadoop fs -rm /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFSyncSITIncremental.properties

hadoop fs -put  /pr/app/ve2/ccp/cogx/phi/gbd/r000/control/application_prd.properties  /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/
hadoop fs -put  /pr/app/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFSyncSITAll.properties  /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/
hadoop fs -put  /pr/app/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFSyncSITIncremental.properties  /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/


hadoop fs -chmod 771 /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/*
--------------------------------------------


Step 6 - Schedule and run once for Sync job
(Use the latest script from Prod)
*** Create Script: BDF_SOURCE2NARROW_FULLLOAD_SPARK.sh ****** (Run once with no Control job created)
-------------------------------------------------
spark2-submit  \
--master yarn  \
--queue ndo_coca_yarn  \
--deploy-mode cluster  \
--executor-memory 40G  \
--executor-cores 8  \
--driver-cores 8  \
--driver-memory 30G  \
--name "Cogx_ETL_BDF_SYNC"  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml  \
--conf spark.sql.codegen.wholeStage=true  \
--conf spark.yarn.maxAppAttempts=1  \
--conf spark.driver.memoryOverhead=5120  \
--conf spark.sql.parquet.cacheMetadata=false  \
--conf spark.yarn.executor.memoryOverhead=16384  \
--conf spark.network.timeout=420000  \
--conf "spark.driver.maxResultSize=15g"  \
--conf "spark.default.parallelism=1000"  \
--conf spark.kryoserializer.buffer.max=1024m  \
--conf spark.rpc.message.maxSize=1024  \
--conf spark.sql.broadcastTimeout=5800  \
--conf spark.executor.heartbeatInterval=30s  \
--conf spark.dynamicAllocation.executorIdleTimeout=90  \
--conf spark.dynamicAllocation.initialExecutors=0  \
--conf spark.dynamicAllocation.maxExecutors=100  \
--conf spark.dynamicAllocation.minExecutors=30  \
--conf spark.sql.autoBroadcastJoinThreshold=-1  \
--conf spark.sql.cbo.enabled=true  \
--conf "spark.yarn.security.tokens.hbase.enabled=true"  \
--conf "spark.sql.shuffle.partitions=4200"  \
--conf spark.dynamicAllocation.enabled=true  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml,/opt/cloudera/parcels/CDH/lib/hbase/conf/hbase-site.xml  \
--keytab /etc/keytabs/srcccpcogxbthpr.keytab  \
--jars /opt/cloudera/parcels/CDH/jars/hive-contrib-1.1.0-cdh5.12.2.jar  \
--jars /usr/lib/tdch/1.5/lib/terajdbc4.jar,/usr/lib/tdch/1.5/lib/tdgssconfig.jar  \
--class com.anthem.cogx.etl.HiveBDFSync.CogxHiveBDFSyncDriver \
hdfs:///pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/ds-cogx-etl-2.0.7.jar  \
/pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/ prd cogxHiveBDFSyncSITAll >$YARN_LOG_SMRY_FILE 2>&1 
--------------------------------------------

Step 7 - Schedule and run daily for daily Sync job
(Use the latest script from Prod)
*** Create Script: BDF_SOURCE2NARROW_SPARK.sh ****** (Need Control M Job)
-------------------------------------------------
spark2-submit  \
--master yarn  \
--queue ndo_coca_yarn  \
--deploy-mode cluster  \
--executor-memory 40G  \
--executor-cores 8  \
--driver-cores 8  \
--driver-memory 30G  \
--name "Cogx_ETL_BDF_SYNC"  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml  \
--conf spark.sql.codegen.wholeStage=true  \
--conf spark.yarn.maxAppAttempts=1  \
--conf spark.driver.memoryOverhead=5120  \
--conf spark.sql.parquet.cacheMetadata=false  \
--conf spark.yarn.executor.memoryOverhead=16384  \
--conf spark.network.timeout=420000  \
--conf "spark.driver.maxResultSize=15g"  \
--conf "spark.default.parallelism=1000"  \
--conf spark.kryoserializer.buffer.max=1024m  \
--conf spark.rpc.message.maxSize=1024  \
--conf spark.sql.broadcastTimeout=5800  \
--conf spark.executor.heartbeatInterval=30s  \
--conf spark.dynamicAllocation.executorIdleTimeout=90  \
--conf spark.dynamicAllocation.initialExecutors=0  \
--conf spark.dynamicAllocation.maxExecutors=100  \
--conf spark.dynamicAllocation.minExecutors=30  \
--conf spark.sql.autoBroadcastJoinThreshold=-1  \
--conf spark.sql.cbo.enabled=true  \
--conf "spark.yarn.security.tokens.hbase.enabled=true"  \
--conf "spark.sql.shuffle.partitions=4200"  \
--conf spark.dynamicAllocation.enabled=true  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml,/opt/cloudera/parcels/CDH/lib/hbase/conf/hbase-site.xml  \
--keytab /etc/keytabs/srcccpcogxbthpr.keytab  \
--jars /opt/cloudera/parcels/CDH/jars/hive-contrib-1.1.0-cdh5.12.2.jar  \
--jars /usr/lib/tdch/1.5/lib/terajdbc4.jar,/usr/lib/tdch/1.5/lib/tdgssconfig.jar  \
--class com.anthem.cogx.etl.HiveBDFSync.CogxHiveBDFSyncDriver \
hdfs:///pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/ds-cogx-etl-2.0.7.jar  \
/pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/ prd cogxHiveBDFSyncSITIncremental >$YARN_LOG_SMRY_FILE 2>&1 
--------------------------------------------

Step 8 - Build the Staging table:
--------------------------------------------------------

CREATE  TABLE IF NOT EXISTS pr_ccpcogxph_gbd_r000_sg.COGX_BDF_STAGING (
rowKey varchar(500) COMMENT '',
jsonData varchar(50000) COMMENT ''
)
stored as parquet
 
---------------------------------------------------------

Step 9 - Create Staging loading Full loading Configuration files

query_cogxHiveBDFHiveHistory.properties
---------------------------------------

hbase_table_name=cogx_claims
hbase_table_columnfamily=c1
hbase_table_columnname=jsonData

### On Linux need Double Quote
hbaseconfigfile="hbase-site.xml"

## Audit Table and Hbase target
teradata_table_name=cogx_claims
ABCSaveFormat=hive
ABCHIVEDBTABLE=<<auditSchema>>.cogx_etl_audit

## Header query
query_clm_wgs_gncclmp="SELECT   hdr.ddc_cd_dcn as ddc_cd_dcn,  hdr.ddc_cd_dcn_cc as ddc_cd_dcn_cc,   hdr.ddc_cd_itm_cde as ddc_cd_itm_cde,   hdr.ddc_cd_clm_compl_dte as ddc_cd_clm_compl_dte,   hdr.ddc_cd_clm_pay_act_1 as ddc_cd_clm_pay_act_1,   hdr.ddc_cd_clm_pay_act_2_6 as ddc_cd_clm_pay_act_2_6,   Concat(hdr.ddc_cd_cert_nbr1, hdr.ddc_cd_cert_nbr2, hdr.ddc_cd_cert_nbr3) as member_id,   hdr.ddc_cd_pat_mbr_cde as ddc_cd_pat_mbr_cde,   hdr.ddc_cd_grp_nbr as ddc_cd_grp_nbr,   hdr.ddc_cd_svc_from_dte as ddc_cd_svc_from_dte,   hdr.ddc_cd_svc_thru_dte as ddc_cd_svc_thru_dte,   hdr.ddc_cd_prvdr_tax_id as ddc_cd_prvdr_tax_id,   hdr.ddc_cd_prvdr_nme as ddc_cd_prvdr_nme,   hdr.ddc_cd_prvdr_sec_nme as ddc_cd_prvdr_sec_nme,   hdr.ddc_cd_prvdr_spclty_cde as ddc_cd_prvdr_spclty_cde,   Concat(hdr.ddc_cd_prvdr_lic_alpha, hdr.ddc_cd_prvdr_lic_nmrc) AS PROV_LCNS_CD,   hdr.ddc_cd_tot_chrg_amt as ddc_cd_tot_chrg_amt,  hdr.ddc_cd_med_rec_nbr_2 as ddc_cd_med_rec_nbr_2,   hdr.ddc_cd_med_rec_nbr as ddc_cd_med_rec_nbr,   hdr.ddc_cd_icda_cde_1 as ddc_cd_icda_cde_1,   hdr.ddc_cd_icda_cde_2 as ddc_cd_icda_cde_2,   hdr.ddc_cd_icda_cde_3 as ddc_cd_icda_cde_3,   hdr.ddc_cd_icda_cde_4 as ddc_cd_icda_cde_4,   hdr.ddc_cd_icda_cde_5 as ddc_cd_icda_cde_5,   CASE   WHEN ( hdr.ddc_cd_its_home_ind = 'Y'   AND hdr.ddc_cd_its_orig_sccf_nbr_new <> '*'   AND hdr.ddc_cd_its_host_prvdr_ind IN ( 'P', 'Y' ) )   OR hdr.ddc_cd_prvdr_ind NOT IN ( 'D', 'N' ) THEN 'PAR'   ELSE 'NON-PAR'   END AS PRVDR_STATUS,   CASE   WHEN hdr.ddc_cd_clm_type IN ( 'MA', 'PA', 'PC', 'MM', 'PM' ) THEN   'PROF'   WHEN hdr.ddc_cd_clm_type IN ( 'IA', 'IC', 'ID' ) THEN 'INPT'   WHEN hdr.ddc_cd_clm_type IN ( 'OA', 'OC', 'OD' ) THEN 'OUTPT'   WHEN hdr.ddc_cd_clm_type IN ( 'SA', 'SC' ) THEN 'SN'   ELSE hdr.ddc_cd_clm_type   END AS CLAIM_TYPE,    hdr.load_dtm as load_dtm , hdr.load_ingstn_id as load_ingstn_id  FROM <<sourceDB>>.clm_wgs_gncclmp_cogx_TEST hdr   WHERE hdr.ddc_cd_itm_cde = '80'   AND hdr.ddc_cd_clm_compl_dte >= <<histDate>>  <<Header_Additional_Filter>>"
Header_Additional_Filter="  "
#Header_Additional_Filter=" and hdr.load_ingstn_id = 20180801 "
#Header_Additional_Filter="   and hdr.ddc_cd_svc_from_dte = 20180814 and hdr.ddc_cd_svc_thru_dte = 20180814 AND hdr.ddc_cd_clm_compl_dte >= 20160607 and hdr.ddc_cd_pat_mbr_cde = 20 "

## Detail query
query_clm_wgs_gncdtlp="select   dtl.gnchiios_hclm_dcn as gnchiios_hclm_dcn,  dtl.gnchiios_hclm_item_cde as gnchiios_hclm_item_cde,  dtl.ddc_dtl_lne_nbr as ddc_dtl_lne_nbr,   dtl.ddc_dtl_icda_pntr_1 as ddc_dtl_icda_pntr_1,   dtl.ddc_dtl_prcdr_cde as ddc_dtl_prcdr_cde,   dtl.ddc_dtl_svc_cde_1_3 as ddc_dtl_svc_cde_1_3,   dtl.ddc_dtl_proc_svc_cls_1 as ddc_dtl_proc_svc_cls_1,   dtl.ddc_dtl_proc_svc_cls_2 as ddc_dtl_proc_svc_cls_2,   dtl.ddc_dtl_proc_svc_cls_3 as ddc_dtl_proc_svc_cls_3,   dtl.ddc_dtl_pcodec_hcpcs_cde as ddc_dtl_pcodec_hcpcs_cde,   dtl.ddc_dtl_blld_amt as ddc_dtl_blld_amt,   dtl.ddc_dtl_unts_occur as ddc_dtl_unts_occur,   dtl.ddc_dtl_units_occur as ddc_dtl_units_occur,   dtl.ddc_dtl_prcdr_modfr_cde as ddc_dtl_prcdr_modfr_cde,   dtl.ddc_dtl_pcodec_hcpcs_mod as ddc_dtl_pcodec_hcpcs_mod,   dtl.ddc_dtl_mod_cde_1 as ddc_dtl_mod_cde_1,   dtl.ddc_dtl_mod_cde_2 as ddc_dtl_mod_cde_2,   dtl.ddc_dtl_mod_cde_3 as ddc_dtl_mod_cde_3,   dtl.ddc_dtl_hcfa_pt_cde as ddc_dtl_hcfa_pt_cde,   dtl.ddc_dtl_pt_cde as ddc_dtl_pt_cde,   dtl.ddc_dtl_elig_expsn_amt as ddc_dtl_elig_expsn_amt,   dtl.ddc_dtl_svc_from_dte AS SRVC_FROM_DT_DTL,   dtl.ddc_dtl_svc_thru_dte AS SRVC_TO_DT_DTL  from <<sourceDB>>.clm_wgs_gncdtlp_cogx_TEST dtl where dtl.gnchiios_hclm_item_cde='80'  <<Detail_Additional_Filter>>" 
Detail_Additional_Filter=" "
#Detail_Additional_Filter=" and dtl.load_ingstn_id = 20180801 "

## EA2 query
query_clm_wgs_gncnatp_ea2="SELECT  EA2.ddc_nat_ea2_type_of_bill,  EA2.gnchiios_hclm_dcn,  EA2.gnchiios_hclm_dcn_cc,  EA2.gnchiios_hclm_item_cde   FROM   <<sourceDB>>.clm_wgs_gncnatp_ea2_cogx_TEST EA2   WHERE EA2.gnchiios_hclm_item_cde = '80' <<ea_Additional_Filter>>"
ea_Additional_Filter=" "
#ea_Additional_Filter=" and EA2.load_ingstn_id = 20180801  "

## roll back period
backout_months=36

# Incremental loading Config
default_incremental_startdt=20190401
isIncremental=No
force_default_incremental=no


NarrowStageDB="<<sourceDB>>.COGX_BDF_STAGING"

truncateSQL=" truncate table <<sourceDB>>.COGX_BDF_STAGING"


---------------------------------------


Step 10 - Create Staging Daily loading Configuration file

query_cogxHiveBDFHiveIncremental.properties
---------------------------------------


hbase_table_name=cogx_claims_new
hbase_table_columnfamily=c1
hbase_table_columnname=jsonData

### On Linux need Double Quote
hbaseconfigfile="hbase-site.xml"

## Audit Table and Hbase target
teradata_table_name=cogx_claims
cogx_audit_query="select max(CAST(date_format(to_date(lastupdate),'yyyyMMdd') as INT)) from <<auditSchema>>.cogx_um_teradata_hb_audit where program='<<programName>>' and status ='completed' "
ABCSaveFormat=hive
ABCHIVEDBTABLE=<<auditSchema>>.cogx_etl_audit

## Header query
#query_clm_wgs_gncclmp=" SELECT   hdr.ddc_cd_dcn as ddc_cd_dcn,  hdr.ddc_cd_dcn_cc as ddc_cd_dcn_cc,  hdr.ddc_cd_itm_cde as ddc_cd_itm_cde,   hdr.ddc_cd_clm_compl_dte as ddc_cd_clm_compl_dte,   hdr.ddc_cd_clm_pay_act_1 as ddc_cd_clm_pay_act_1,   hdr.ddc_cd_clm_pay_act_2_6 as ddc_cd_clm_pay_act_2_6,   Concat(hdr.ddc_cd_cert_nbr1,ddc_cd_cert_nbr2,hdr.ddc_cd_cert_nbr3) AS member_id,  hdr.ddc_cd_pat_mbr_cde as ddc_cd_pat_mbr_cde,   hdr.ddc_cd_grp_nbr as ddc_cd_grp_nbr,   hdr.ddc_cd_svc_from_dte as ddc_cd_svc_from_dte,   hdr.ddc_cd_svc_thru_dte as ddc_cd_svc_thru_dte,   hdr.ddc_cd_prvdr_tax_id as ddc_cd_prvdr_tax_id,   hdr.ddc_cd_prvdr_nme as ddc_cd_prvdr_nme,   hdr.ddc_cd_prvdr_sec_nme as ddc_cd_prvdr_sec_nme,   hdr.ddc_cd_prvdr_spclty_cde as ddc_cd_prvdr_spclty_cde,   Concat(hdr.ddc_cd_prvdr_lic_alpha, ddc_cd_prvdr_lic_nmrc) AS prov_lcns_cd,   hdr.ddc_cd_tot_chrg_amt as ddc_cd_tot_chrg_amt,   hdr.ddc_cd_med_rec_nbr_2 as ddc_cd_med_rec_nbr_2,   hdr.ddc_cd_med_rec_nbr as ddc_cd_med_rec_nbr,   hdr.ddc_cd_icda_cde_1 as ddc_cd_icda_cde_1,   hdr.ddc_cd_icda_cde_2 as ddc_cd_icda_cde_2,   hdr.ddc_cd_icda_cde_3 as ddc_cd_icda_cde_3,   hdr.ddc_cd_icda_cde_4 as ddc_cd_icda_cde_4,   hdr.ddc_cd_icda_cde_5 as ddc_cd_icda_cde_5,   CASE   WHEN ( hdr.ddc_cd_its_home_ind = 'Y'   AND hdr.ddc_cd_its_orig_sccf_nbr_new <> '*'   AND hdr.ddc_cd_its_host_prvdr_ind IN ( 'P' ,'Y' ) )   OR hdr.ddc_cd_prvdr_ind NOT IN ( 'D', 'N' ) THEN 'PAR'   ELSE 'NON-PAR'   END   AS prvdr_status,  CASE   WHEN hdr.ddc_cd_clm_type IN ( 'MA' ,'PA' ,'PC', 'MM' ,'PM' ) THEN   'PROF'   WHEN hdr.ddc_cd_clm_type IN ( 'IA', 'IC', 'ID' ) THEN 'INPT'   WHEN hdr.ddc_cd_clm_type IN ( 'OA', 'OC', 'OD' ) THEN 'OUTPT'   WHEN hdr.ddc_cd_clm_type IN ( 'SA', 'SC' ) THEN 'SN'   ELSE hdr.ddc_cd_clm_type   END   AS claim_type,  hdr.load_dtm as load_dtm,  hdr.load_ingstn_id as load_ingstn_id  FROM <<sourceDB>>.clm_wgs_gncclmp hdr  INNER JOIN (SELECT DISTINCT ddc_cd_dcn from <<sourceDB>>.clm_wgs_gncclmp   WHERE load_ingstn_id > <<load_date>> AND ddc_cd_itm_cde = '80')X  ON hdr.ddc_cd_dcn = x.ddc_cd_dcn  AND hdr.ddc_cd_itm_cde = '80'  AND hdr.ddc_cd_clm_compl_dte >= <<histDate>>  <<Header_Additional_Filter>>  "
query_clm_wgs_gncclmp=" SELECT   hdr.ddc_cd_dcn as ddc_cd_dcn,  hdr.ddc_cd_dcn_cc as ddc_cd_dcn_cc,  hdr.ddc_cd_itm_cde as ddc_cd_itm_cde,   hdr.ddc_cd_clm_compl_dte as ddc_cd_clm_compl_dte,   hdr.ddc_cd_clm_pay_act_1 as ddc_cd_clm_pay_act_1,   hdr.ddc_cd_clm_pay_act_2_6 as ddc_cd_clm_pay_act_2_6,   Concat(hdr.ddc_cd_cert_nbr1,ddc_cd_cert_nbr2,hdr.ddc_cd_cert_nbr3) AS member_id,  hdr.ddc_cd_pat_mbr_cde as ddc_cd_pat_mbr_cde,   hdr.ddc_cd_grp_nbr as ddc_cd_grp_nbr,   hdr.ddc_cd_svc_from_dte as ddc_cd_svc_from_dte,   hdr.ddc_cd_svc_thru_dte as ddc_cd_svc_thru_dte,   hdr.ddc_cd_prvdr_tax_id as ddc_cd_prvdr_tax_id,   hdr.ddc_cd_prvdr_nme as ddc_cd_prvdr_nme,   hdr.ddc_cd_prvdr_sec_nme as ddc_cd_prvdr_sec_nme,   hdr.ddc_cd_prvdr_spclty_cde as ddc_cd_prvdr_spclty_cde,   Concat(hdr.ddc_cd_prvdr_lic_alpha, ddc_cd_prvdr_lic_nmrc) AS prov_lcns_cd,   hdr.ddc_cd_tot_chrg_amt as ddc_cd_tot_chrg_amt,   hdr.ddc_cd_med_rec_nbr_2 as ddc_cd_med_rec_nbr_2,   hdr.ddc_cd_med_rec_nbr as ddc_cd_med_rec_nbr,   hdr.ddc_cd_icda_cde_1 as ddc_cd_icda_cde_1,   hdr.ddc_cd_icda_cde_2 as ddc_cd_icda_cde_2,   hdr.ddc_cd_icda_cde_3 as ddc_cd_icda_cde_3,   hdr.ddc_cd_icda_cde_4 as ddc_cd_icda_cde_4,   hdr.ddc_cd_icda_cde_5 as ddc_cd_icda_cde_5,   CASE   WHEN ( hdr.ddc_cd_its_home_ind = 'Y'   AND hdr.ddc_cd_its_orig_sccf_nbr_new <> '*'   AND hdr.ddc_cd_its_host_prvdr_ind IN ( 'P' ,'Y' ) )   OR hdr.ddc_cd_prvdr_ind NOT IN ( 'D', 'N' ) THEN 'PAR'   ELSE 'NON-PAR'   END   AS prvdr_status,  CASE   WHEN hdr.ddc_cd_clm_type IN ( 'MA' ,'PA' ,'PC', 'MM' ,'PM' ) THEN   'PROF'   WHEN hdr.ddc_cd_clm_type IN ( 'IA', 'IC', 'ID' ) THEN 'INPT'   WHEN hdr.ddc_cd_clm_type IN ( 'OA', 'OC', 'OD' ) THEN 'OUTPT'   WHEN hdr.ddc_cd_clm_type IN ( 'SA', 'SC' ) THEN 'SN'   ELSE hdr.ddc_cd_clm_type   END   AS claim_type,  hdr.load_dtm as load_dtm,  hdr.load_ingstn_id as load_ingstn_id  FROM <<sourceDB>>.clm_wgs_gncclmp_cogx_TEST hdr   WHERE hdr.load_ingstn_id >= <<load_date>>   AND hdr.ddc_cd_itm_cde = '80'  AND hdr.ddc_cd_clm_compl_dte >= <<histDate>>  <<Header_Additional_Filter>>  "


Header_Additional_Filter=" "

## Detail query
query_clm_wgs_gncdtlp="select   dtl.gnchiios_hclm_dcn as gnchiios_hclm_dcn,  dtl.gnchiios_hclm_item_cde as gnchiios_hclm_item_cde,  dtl.ddc_dtl_lne_nbr as ddc_dtl_lne_nbr,   dtl.ddc_dtl_icda_pntr_1 as ddc_dtl_icda_pntr_1,   dtl.ddc_dtl_prcdr_cde as ddc_dtl_prcdr_cde,   dtl.ddc_dtl_svc_cde_1_3 as ddc_dtl_svc_cde_1_3,   dtl.ddc_dtl_proc_svc_cls_1 as ddc_dtl_proc_svc_cls_1,   dtl.ddc_dtl_proc_svc_cls_2 as ddc_dtl_proc_svc_cls_2,   dtl.ddc_dtl_proc_svc_cls_3 as ddc_dtl_proc_svc_cls_3,   dtl.ddc_dtl_pcodec_hcpcs_cde as ddc_dtl_pcodec_hcpcs_cde,   dtl.ddc_dtl_blld_amt as ddc_dtl_blld_amt,   dtl.ddc_dtl_unts_occur as ddc_dtl_unts_occur,   dtl.ddc_dtl_units_occur as ddc_dtl_units_occur,   dtl.ddc_dtl_prcdr_modfr_cde as ddc_dtl_prcdr_modfr_cde,   dtl.ddc_dtl_pcodec_hcpcs_mod as ddc_dtl_pcodec_hcpcs_mod,   dtl.ddc_dtl_mod_cde_1 as ddc_dtl_mod_cde_1,   dtl.ddc_dtl_mod_cde_2 as ddc_dtl_mod_cde_2,   dtl.ddc_dtl_mod_cde_3 as ddc_dtl_mod_cde_3,   dtl.ddc_dtl_hcfa_pt_cde as ddc_dtl_hcfa_pt_cde,   dtl.ddc_dtl_pt_cde as ddc_dtl_pt_cde,   dtl.ddc_dtl_elig_expsn_amt as ddc_dtl_elig_expsn_amt,   dtl.ddc_dtl_svc_from_dte AS SRVC_FROM_DT_DTL,   dtl.ddc_dtl_svc_thru_dte AS SRVC_TO_DT_DTL  from <<sourceDB>>.clm_wgs_gncdtlp_cogx_TEST dtl where dtl.gnchiios_hclm_item_cde='80'  <<Detail_Additional_Filter>>" 
Detail_Additional_Filter=" "

## EA2 query
query_clm_wgs_gncnatp_ea2="SELECT  EA2.ddc_nat_ea2_type_of_bill,  EA2.gnchiios_hclm_dcn,  EA2.gnchiios_hclm_dcn_cc,  EA2.gnchiios_hclm_item_cde   FROM   <<sourceDB>>.clm_wgs_gncnatp_ea2_cogx_TEST EA2   WHERE EA2.gnchiios_hclm_item_cde = '80' <<ea_Additional_Filter>>"
ea_Additional_Filter=" "

## roll back period
backout_months=36

# Incremental loading Config
default_incremental_startdt=20190521
isIncremental=yes
force_default_incremental=yes

NarrowStageDB="<<sourceDB>>.COGX_BDF_STAGING"

truncateSQL=" truncate table <<sourceDB>>.COGX_BDF_STAGING"


-----------------------------------------

Step 11 - Copy configuration files to Production server
---------------------------------------------------

hadoop fs -rm /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/ds-cogx-etl-2.0.7.jar
hadoop fs -put  /pr/app/ve2/ccp/cogx/phi/gbd/r000/bin/ds-cogx-etl-2.0.7.jar   /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/


hadoop fs -rm /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFHiveIncremental.properties
hadoop fs -rm /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFHiveHistory.properties



hadoop fs -put  /pr/app/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFHiveIncremental.properties  /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/
hadoop fs -put  /pr/app/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFHiveHistory.properties  /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/


hadoop fs -chmod 771 /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/*
---------------------------------------------------


Step 12 - Run one time Full staging table loading
*** Create Script: BDF_NARROW2STAGE_FullLOAD_SPARK.sh ****** (Run Once no need for Control M)
---------------------------------------

spark2-submit  \
--master yarn  \
--queue ndo_coca_yarn  \
--deploy-mode cluster  \
--executor-memory 40G  \
--executor-cores 8  \
--driver-cores 8  \
--driver-memory 30G  \
--name "Cogx_ETL_BDF_Staging"  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml  \
--conf spark.sql.codegen.wholeStage=true  \
--conf spark.yarn.maxAppAttempts=1  \
--conf spark.driver.memoryOverhead=5120  \
--conf spark.sql.parquet.cacheMetadata=false  \
--conf spark.yarn.executor.memoryOverhead=16384  \
--conf spark.network.timeout=420000  \
--conf "spark.driver.maxResultSize=15g"  \
--conf "spark.default.parallelism=1000"  \
--conf spark.kryoserializer.buffer.max=1024m  \
--conf spark.rpc.message.maxSize=1024  \
--conf spark.sql.broadcastTimeout=5800  \
--conf spark.executor.heartbeatInterval=30s  \
--conf spark.dynamicAllocation.executorIdleTimeout=90  \
--conf spark.dynamicAllocation.initialExecutors=0  \
--conf spark.dynamicAllocation.maxExecutors=100  \
--conf spark.dynamicAllocation.minExecutors=30  \
--conf spark.sql.autoBroadcastJoinThreshold=-1  \
--conf spark.sql.cbo.enabled=true  \
--conf "spark.yarn.security.tokens.hbase.enabled=true"  \
--conf "spark.sql.shuffle.partitions=4200"  \
--conf spark.dynamicAllocation.enabled=true  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml,/opt/cloudera/parcels/CDH/lib/hbase/conf/hbase-site.xml  \
--keytab /etc/keytabs/srcccpcogxbthpr.keytab  \
--jars /opt/cloudera/parcels/CDH/jars/hive-contrib-1.1.0-cdh5.12.2.jar  \
--jars /usr/lib/tdch/1.5/lib/terajdbc4.jar,/usr/lib/tdch/1.5/lib/tdgssconfig.jar  \
--class com.anthem.cogx.etl.HiveBDFHive.CogxHiveBDFHiveDriver \
hdfs:///pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/ds-cogx-etl-2.0.7.jar  \
/pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/ prd cogxHiveBDFHiveHistory >$YARN_LOG_SMRY_FILE 2>&1 

-----------------------------------------

Step 13 - Run daily Staging file loading
*** Create Script: BDF_NARROW2STAGE_SPARK.sh ****** (Need to create new Control M)
---------------------------------------

spark2-submit  \
--master yarn  \
--queue ndo_coca_yarn  \
--deploy-mode cluster  \
--executor-memory 40G  \
--executor-cores 8  \
--driver-cores 8  \
--driver-memory 30G  \
--name "Cogx_ETL_BDF_Staging"  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml  \
--conf spark.sql.codegen.wholeStage=true  \
--conf spark.yarn.maxAppAttempts=1  \
--conf spark.driver.memoryOverhead=5120  \
--conf spark.sql.parquet.cacheMetadata=false  \
--conf spark.yarn.executor.memoryOverhead=16384  \
--conf spark.network.timeout=420000  \
--conf "spark.driver.maxResultSize=15g"  \
--conf "spark.default.parallelism=1000"  \
--conf spark.kryoserializer.buffer.max=1024m  \
--conf spark.rpc.message.maxSize=1024  \
--conf spark.sql.broadcastTimeout=5800  \
--conf spark.executor.heartbeatInterval=30s  \
--conf spark.dynamicAllocation.executorIdleTimeout=90  \
--conf spark.dynamicAllocation.initialExecutors=0  \
--conf spark.dynamicAllocation.maxExecutors=100  \
--conf spark.dynamicAllocation.minExecutors=30  \
--conf spark.sql.autoBroadcastJoinThreshold=-1  \
--conf spark.sql.cbo.enabled=true  \
--conf "spark.yarn.security.tokens.hbase.enabled=true"  \
--conf "spark.sql.shuffle.partitions=4200"  \
--conf spark.dynamicAllocation.enabled=true  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml,/opt/cloudera/parcels/CDH/lib/hbase/conf/hbase-site.xml  \
--keytab /etc/keytabs/srcccpcogxbthpr.keytab  \
--jars /opt/cloudera/parcels/CDH/jars/hive-contrib-1.1.0-cdh5.12.2.jar  \
--jars /usr/lib/tdch/1.5/lib/terajdbc4.jar,/usr/lib/tdch/1.5/lib/tdgssconfig.jar  \
--class com.anthem.cogx.etl.HiveBDFHive.CogxHiveBDFHiveDriver \
hdfs:///pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/ds-cogx-etl-2.0.7.jar  \
/pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/ prd cogxHiveBDFHiveIncremental >$YARN_LOG_SMRY_FILE 2>&1 

-----------------------------------------




Step 14 - HBase Truncate Production table

hbase shell
truncation 'pr_hb_ccpcogx_gbd_r000_wh.cogx_claims'

Step 15 - create query_cogxHiveBDFHbase.properties
(Modify based on the latest bitbucket)
------------------------------------------


hbase_table_name=cogx_claims
hbase_table_columnfamily=c1
hbase_table_columnname=jsonData

### On Linux need Double Quote
hbaseconfigfile="hbase-site.xml"

## Audit Table and Hbase target
teradata_table_name=cogx_claims
ABCSaveFormat=hive
ABCHIVEDBTABLE=<<auditSchema>>.cogx_etl_audit

stageSQL="select rowKey, jsonData from <<sourceDB>>.COGX_BDF_STAGING"

---------------------------------------------



Step 16 - Deploy New have Properties

---------------------------------------------------
hadoop fs -rm /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFHbase.properties

hadoop fs -put  /pr/app/ve2/ccp/cogx/phi/gbd/r000/control/query_cogxHiveBDFHbase.properties  /pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/

-----------------------------------------------------



Step 17 -  Schedule and run once for Full load job / This same job can also run as daily run
(Use the latest script from Prod)

*** Create Script: BDF_DUP_INCR_SPARK.sh ****** (Exiting Control M with updated Script)
-------------------------------------------------
spark2-submit  \
--master yarn  \
--queue ndo_coca_yarn  \
--deploy-mode cluster  \
--executor-memory 40G  \
--executor-cores 8  \
--driver-cores 8  \
--driver-memory 30G  \
--name "Cogx_ETL_BDF_Load_HBASE"  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml  \
--conf spark.sql.codegen.wholeStage=true  \
--conf spark.yarn.maxAppAttempts=1  \
--conf spark.driver.memoryOverhead=5120  \
--conf spark.sql.parquet.cacheMetadata=false  \
--conf spark.yarn.executor.memoryOverhead=16384  \
--conf spark.network.timeout=420000  \
--conf "spark.driver.maxResultSize=15g"  \
--conf "spark.default.parallelism=1000"  \
--conf spark.kryoserializer.buffer.max=1024m  \
--conf spark.rpc.message.maxSize=1024  \
--conf spark.sql.broadcastTimeout=5800  \
--conf spark.executor.heartbeatInterval=30s  \
--conf spark.dynamicAllocation.executorIdleTimeout=90  \
--conf spark.dynamicAllocation.initialExecutors=0  \
--conf spark.dynamicAllocation.maxExecutors=100  \
--conf spark.dynamicAllocation.minExecutors=30  \
--conf spark.sql.autoBroadcastJoinThreshold=-1  \
--conf spark.sql.cbo.enabled=true  \
--conf "spark.yarn.security.tokens.hbase.enabled=true"  \
--conf "spark.sql.shuffle.partitions=4200"  \
--conf spark.dynamicAllocation.enabled=true  \
--files /etc/alternatives/spark2-conf/yarn-conf/hive-site.xml,/opt/cloudera/parcels/CDH/lib/hbase/conf/hbase-site.xml  \
--keytab /etc/keytabs/srcccpcogxbthpr.keytab  \
--jars /opt/cloudera/parcels/CDH/jars/hive-contrib-1.1.0-cdh5.12.2.jar  \
--jars /usr/lib/tdch/1.5/lib/terajdbc4.jar,/usr/lib/tdch/1.5/lib/tdgssconfig.jar  \
--class com.anthem.cogx.etl.HiveBDFHBase.CogxHiveBDFHBaseDriver \
hdfs:///pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/ds-cogx-etl-2.0.6.jar  \
/pr/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/ prd cogxHiveBDFHbase >$YARN_LOG_SMRY_FILE 2>&1 
--------------------------------------------


		
		

