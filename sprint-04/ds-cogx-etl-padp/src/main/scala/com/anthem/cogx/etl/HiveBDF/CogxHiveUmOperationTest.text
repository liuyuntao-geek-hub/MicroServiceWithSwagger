package com.anthem.cogx.etl.HiveBDF
import java.io.File

import com.anthem.cogx.etl.config.CogxConfigKey
import com.anthem.cogx.etl.helper.{ CogxOperationSession, CogxOperator }
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions.{ current_timestamp, lit }
import org.apache.spark.sql.functions._
import org.apache.hadoop.security.alias.CredentialProviderFactory
import com.anthem.cogx.etl.helper.CogxOperator
import org.apache.spark.sql.types.StringType
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.PairRDDFunctions
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import java.io.PrintWriter
import java.io.StringWriter
import org.apache.commons.codec.digest.DigestUtils
import com.anthem.cogx.etl.helper.CogxClaimRecord
import com.anthem.cogx.etl.helper.cogxClaimInfo
import com.anthem.cogx.etl.helper.cogxClaimHistory
import collection.JavaConverters._
import java.io.PrintWriter
import java.io.StringWriter
import com.anthem.cogx.etl.util.CogxCommonUtils.asJSONString
//import com.anthem.cogx.etl.util.CogxCommonUtils.getMapReduceJobConfiguration
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.fs.{ FSDataInputStream, Path }
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.HBaseAdmin
import java.io.InputStream
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.storage.StorageLevel
import org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._
import com.anthem.cogx.etl.util.CogxDateUtils


class CogxHiveBDFOperationTest(confFilePath: String, env: String, queryFileCategory: String) extends CogxOperationSession(confFilePath, env, queryFileCategory) with CogxOperator  {

	sc.setLogLevel("info")

	import spark.sql
	import spark.implicits._
	val repartitionNum = config.getInt("repartitionNum")
	val teradata_table_name = config.getString("teradata_table_name")
	var rowCount: Long = 0l
	val columnFamily = config.getString("hbase_table_columnfamily")
	val columnName = config.getString("hbase_table_columnname")
	

	def loadData(): Map[String, DataFrame] = {	info(s"[COGX] Reading the queries from config file")

			/* Read queryies from Configuration file */  
			val backout_months = config.getInt("backout_months")
	  
	    val histDate:String = CogxDateUtils.addMonthToDateWithFormat(CogxDateUtils.getCurrentDateWithFormat("yyyyMMdd"),"yyyyMMdd", -(backout_months))

			/* 1. Claim Header */
			val clmwgsgncclmpSQL =  config.getString("query_clm_wgs_gncclmp").
			replace("<<sourceDB>>", config.getString("inbound-hive-db")).
			replace("<<Header_Additional_Filter>>",  config.getString("Header_Additional_Filter")).
			replace("<<histDate>>", histDate )
			info("clmwgsgncclmpSQL SQL: " + clmwgsgncclmpSQL)
			val clmHeaderDFTrim = spark.sql(clmwgsgncclmpSQL).repartition(repartitionNum)
/*		  val clmHeaderDFTrim = clmHeaderDF.columns.foldLeft(clmHeaderDF){(memoDF,colName)=> memoDF.withColumn(colName, trim(col(colName)))}
		     df.schema(colName).dataType match {
              case StringType => { //info(s"%%%%%%% TRIMMING COLUMN ${colName.toLowerCase()} %%%%%% VALUE '${trim(col(colName))}'"); 
              df.withColumn(colName.toLowerCase, trim(col(colName))); }
              case _ => {// info("Column " + colName.toLowerCase() + " is not being trimmed"); 
              df.withColumn(colName.toLowerCase, col(colName)); }
            }
          }*/
		  //memoDF.withColumn(colName, trim(col(colName)))}
			//info( "  ================== Cogx Performance Test clmHeaderDF: " +  clmHeaderDF.count())


			/* 2. Claim Detail */
			val clmwgsgncdtlpSQL =   config.getString("query_clm_wgs_gncdtlp").replace("<<sourceDB>>", config.getString("inbound-hive-db")).replace("<<Detail_Additional_Filter>>",  config.getString("Detail_Additional_Filter"))
			info("clmwgsgncdtlpSQL SQL: " + clmwgsgncdtlpSQL)
			val clmDetailDFTrim = spark.sql(clmwgsgncdtlpSQL).repartition(repartitionNum)
/*			val  clmDetailDFTrim = clmDetailDF.columns.foldLeft(clmDetailDF){(df,colName)=> 
            df.schema(colName).dataType match {
              case StringType => { //info(s"%%%%%%% TRIMMING COLUMN ${colName.toLowerCase()} %%%%%% VALUE '${trim(col(colName))}'"); 
              df.withColumn(colName.toLowerCase, trim(col(colName))); }
              case _ => {// info("Column " + colName.toLowerCase() + " is not being trimmed"); 
              df.withColumn(colName.toLowerCase, col(colName)); }
            }
          }*/
			//info( "  ================== Cogx Performance Test clmDetailDF: " +  clmDetailDF.count())


			/* 3. EA2 */
			val clmwgsgncnatpea2SQL =   config.getString("query_clm_wgs_gncnatp_ea2").replace("<<sourceDB>>", config.getString("inbound-hive-db")).replace("<<ea_Additional_Filter>>",  config.getString("ea_Additional_Filter"))
			info("clmwgsgncnatpea2SQL SQL: " + clmwgsgncnatpea2SQL)
			val clmgncnatpea2DFTrim = spark.sql(clmwgsgncnatpea2SQL).repartition(repartitionNum)
			/*val clmgncnatpea2DFTrim = clmgncnatpea2DF.columns.foldLeft(clmgncnatpea2DF){(df,colName)=> 
            df.schema(colName).dataType match {
              case StringType => { //info(s"%%%%%%% TRIMMING COLUMN ${colName.toLowerCase()} %%%%%% VALUE '${trim(col(colName))}'"); 
              df.withColumn(colName.toLowerCase, trim(col(colName))); }
              case _ => {// info("Column " + colName.toLowerCase() + " is not being trimmed"); 
              df.withColumn(colName.toLowerCase, col(colName)); }
            }
          }*/
			//info( "  ================== Cogx Performance Test clmgncnatpea2DF: " +  clmgncnatpea2DF.count())



			/*Creating a map of table name and respective dataframes */

			val dataMap = Map("clm_wgs_gncclmp" -> clmHeaderDFTrim,
					"clm_wgs_gncdtlp" -> clmDetailDFTrim,
					"clm_wgs_gncnatp_ea2" -> clmgncnatpea2DFTrim)


			return dataMap
	}


	def processData(inDFs: Map[String, DataFrame]): Map[String, DataFrame] = {


			    val clmHeaderDF = inDFs.getOrElse("clm_wgs_gncclmp", null)
					val clmDetailDF = inDFs.getOrElse("clm_wgs_gncdtlp", null)
					val clmgncnatpea2DF = inDFs.getOrElse("clm_wgs_gncnatp_ea2", null)

					var bdfCombineClaimsDF = clmHeaderDF.as("header").join(clmDetailDF.as("detail"), $"header.ddc_cd_dcn" === $"gnchiios_hclm_dcn", "inner")
					.join(clmgncnatpea2DF.as("ea2"), $"header.ddc_cd_dcn" === $"ea2.gnchiios_hclm_dcn" and $"header.ddc_cd_dcn_cc" === $"ea2.gnchiios_hclm_dcn_cc", "inner")
					.select("header.ddc_cd_dcn", "header.ddc_cd_itm_cde", "header.ddc_cd_clm_compl_dte", "header.ddc_cd_clm_pay_act_1", "header.ddc_cd_clm_pay_act_2_6", "header.member_id",
							"header.ddc_cd_pat_mbr_cde", "header.ddc_cd_grp_nbr", "header.ddc_cd_svc_from_dte", "header.ddc_cd_svc_thru_dte", "header.ddc_cd_prvdr_tax_id",
							"header.ddc_cd_prvdr_nme", "header.ddc_cd_prvdr_sec_nme", "header.ddc_cd_prvdr_spclty_cde", "header.prov_lcns_cd", "header.ddc_cd_tot_chrg_amt",
							"ea2.ddc_nat_ea2_type_of_bill", "header.ddc_cd_med_rec_nbr_2", "header.ddc_cd_med_rec_nbr", "header.ddc_cd_icda_cde_1", "header.ddc_cd_icda_cde_2",
							"header.ddc_cd_icda_cde_3", "header.ddc_cd_icda_cde_4", "header.ddc_cd_icda_cde_5", "header.prvdr_status", "header.claim_type",
							"detail.ddc_dtl_lne_nbr", "detail.ddc_dtl_icda_pntr_1", "detail.ddc_dtl_prcdr_cde", "detail.ddc_dtl_svc_cde_1_3", "detail.ddc_dtl_proc_svc_cls_1",
							"detail.ddc_dtl_proc_svc_cls_2", "detail.ddc_dtl_proc_svc_cls_3", "detail.ddc_dtl_pcodec_hcpcs_cde", "detail.ddc_dtl_blld_amt", "detail.ddc_dtl_unts_occur",
							"detail.ddc_dtl_units_occur", "detail.ddc_dtl_prcdr_modfr_cde", "detail.ddc_dtl_pcodec_hcpcs_mod", "detail.ddc_dtl_mod_cde_1", "detail.ddc_dtl_mod_cde_2",
							"detail.ddc_dtl_mod_cde_3", "detail.ddc_dtl_hcfa_pt_cde", "detail.ddc_dtl_pt_cde", "detail.ddc_dtl_elig_expsn_amt", "detail.srvc_from_dt_dtl",
							"detail.srvc_to_dt_dtl", "header.load_ingstn_id", "header.load_dtm").repartition(repartitionNum)
							
					//info( "  ================== Cogx Performance Test bdfCombineClaimsDF: " +  bdfCombineClaimsDF.count())

					var bdfClaimsPartition = Window.partitionBy("ddc_cd_dcn", "ddc_cd_itm_cde", "ddc_dtl_lne_nbr").orderBy(col("load_dtm").desc)
					bdfCombineClaimsDF = bdfCombineClaimsDF.withColumn("rank", rank().over(bdfClaimsPartition)).filter("rank=1")

					//rowCount = bdfCombineClaimsDF.count()
					//info(s"[COGX]INFO: CogX Row Count => " + rowCount)

					var CogaClaimDataset = bdfCombineClaimsDF.as[CogxClaimRecord]

					//info( "  ================== Cogx Performance Test CogaClaimDataset: " +  CogaClaimDataset.count())

       
			    val ArraySizeLimit = config.getInt("ArraySizeLimit")
			    val reduce_function = (a: Set[CogxClaimRecord],b: Set[CogxClaimRecord])=>{
                                         if (a.size <=ArraySizeLimit)  
                                           {a ++ b}
                                         else{print("======== Oversized member (over size 3000):" + a.last.member_id);a}
                                         }
			    
			    
          val RDDSet = CogaClaimDataset.rdd.repartition(repartitionNum).map(record => ((record.member_id, record.ddc_cd_pat_mbr_cde, record.ddc_cd_svc_from_dte, record.ddc_cd_svc_thru_dte),Set(record))).reduceByKey(reduce_function).repartition(repartitionNum)
                                         
          //println ("RDDset Count:" + RDDSet.count())
          
          var DSSet = RDDSet.map(k=>{( new StringBuilder((DigestUtils.md5Hex(String.valueOf(k._1))).substring(0, 8)).append(String.valueOf(k._1).replaceAll("\\(", "").replaceAll("\\)", "")).toString(),asJSONString(new cogxClaimHistory(k._2.toArray)))}).repartition(repartitionNum)
          
          //println ("DSSet count: " + DSSet.count())
          var newDF= DSSet.toDF("rowKey", "jsonData").repartition(repartitionNum)
          
          println("New DataSet: ")
          println("===========================================")
          //println("Total New Count: "+newDF.count())
          
    
          var dataMap = Map(teradata_table_name->newDF)
          return dataMap

	}


	def writeData(outDFs: Map[String, DataFrame]): Unit = {

			val df1 = outDFs.getOrElse(teradata_table_name, null).toDF("rowKey", "jsonData")

					 df1.persist(StorageLevel.MEMORY_AND_DISK).count
					//df1.show()
					val hbaseCount = df1.count()
					println("Loading to HBase count: "+ hbaseCount)

					val columnFamily = config.getString("hbase_table_columnfamily")
					val columnName = config.getString("hbase_table_columnname")
					val putRDD = df1.rdd.map(x => {
						val rowKey = x.getAs[String]("rowKey")
								val holder = x.getAs[String]("jsonData")
								//(rowKey, holder)
								val p = new Put(Bytes.toBytes(rowKey))
								p.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName), Bytes.toBytes(holder))
								(new org.apache.hadoop.hbase.io.ImmutableBytesWritable, p)
					}
							)

					val hbaseTable = config.getString("hbase_schema") + ":" + config.getString("hbase_table_name")

					if (env.equalsIgnoreCase("local"))
					{
						//// Run it on local ////
						getMapReduceJobConfiguration(hbaseTable)
					}
					else
					{
						/// Run it on Cluster now
						new PairRDDFunctions(putRDD).saveAsNewAPIHadoopDataset(getMapReduceJobConfiguration(hbaseTable))
						//info("Loadde to HBase count: "+ hbaseCount)
					}


	}
	
}