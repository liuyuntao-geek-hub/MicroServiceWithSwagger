/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_STATIC_LOAD_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control wgsp_pcp_vbp_prgm_mapping

Command
/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_STATIC_LOAD_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control wgsp_pcp_vbp_prgm_mapping
/ts/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_STATIC_LOAD_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control wgsp_pcp_vbp_prgm_mapping

${USER_NAME}@US.AD.WELLPOINT.COM

https://bdpr3r7mn4pr.wellpoint.com:8090/cluster/app/application_1537729904686_12637
https://bdpr3r9mn5pr.wellpoint.com:19890/jobhistory/logs//bdpr3r3w5pr.wellpoint.com:8041/container_e60_1537729904686_12637_01_000001/container_e60_1537729904686_12637_01_000001/srcpdpspcpbthpr

https://bdpr3r7mn4pr.wellpoint.com:8090/cluster/app/application_1537729904686_12637
https://bdpr3r7mn4pr.wellpoint.com:8090/cluster/app/application_1537729904686_15063

https://bdpr3r7mn4pr.wellpoint.com:8090/cluster/app/application_1537729904686_15056/
https://bdpr3r7mn4pr.wellpoint.com:8090/cluster/app/application_1537729904686_15143

https://bdpr3r7mn4pr.wellpoint.com:8090/cluster/app/application_1537729904686_16823

https://bdpr3r7mn4pr.wellpoint.com:8090/cluster/app/application_1537729904686_18848


https://bdpr3r7mn4pr.wellpoint.com:8090/cluster/app/application_1540739177921_4037/

https://bdpr3r7mn4pr.wellpoint.com:8090/proxy/application_1537729904686_16904/ 
-Dsplice.spark.local.dir=/tmp 

 hdfs dfs -getfacl /tmp/splice


yarn logs -applicationId application_1537729904686_15143 |


sqlshell.sh -h bdpr3splice.wellpoint.com -u srcpdpspcpbthpr -P 

/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_STATIC_LOAD_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control wgsp_pcp_vbp_prgm_mapping
/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_PROVIDER_VBP_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control
/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_PRODUCT_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control
/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_EXPORT_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control prod_pkg
/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_EXPORT_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control prod_plan

/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_EXPORT_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control prod_pkg
/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_EXPORT_Wrapper.sh prod /pr/app/ve2/pdp/spcp/phi/no_gbd/r000/control prod_plan



SPCP_CDL_MBR_INFO_LOAD_WRAPPER_0001 - File=/bin/sh $CODE_NOGBD/scripts/spcp_member_info.lst 
SPCP_CDL_PRODUCT_TYPE_LOAD_WRAPPER_0001 -/bin/sh $CODE_NOGBD/scripts/spcp_product.lst 


------------------------------------------
spcp-etl-export_prod_plan_20181005154844_srcpdpspcpbthpr.log
Exception in thread "main" org.apache.hadoop.security.KerberosAuthException: Login failure for user: srcpdpspcpbthpr@DEVAD.WELLPOINT.COM from keytab /home/srcpdpspcpbthpr/srcpdpspcpbthpr.keytab javax.security.auth.login.LoginException: Unable to obtain password from user

        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1130)
        at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:579)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user

        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:897)
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760)
        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)
        at javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1121)
        ... 4 more
/pr/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/SPCP_EXPORT_Wrapper.sh: line 116: [: ==: unary operator expected

------------------------------------------

-- Language

delete from ts_pdpspcp_xm.wgsp_vbp_prgm_mapping;

call SYSCS_UTIL.IMPORT_DATA (
'pdpspcp_xm',
'WGSP_PCP_VBP_PRGM_MAPPING',
null,
'hdfs:///user/srcpdpspcpbthpr/flat_files/vbp_program_codes.txt',
'|',
null,
'yyyy-MM-dd HH:mm:ss.SSZ',
'yyyy-MM-dd',
null,
'25',
'hdfs:///user/srcpdpspcpbthts/bad_records',
true,
null
);

exit

=====================================================



PATH=$PATH:$HOME/bin

export PATH


export SPCP_EDGE_ROOT="/ts/app/ve2/pdp/spcp/phi/no_gbd/r000"
export SPCP_HDFS_ROOT="/ts/hdfsapp/ve2/pdp/spcp/phi/no_gbd/r000"
export SPCP_HDFS_USER_HOME="/user/srcpdpspcpbthts"
export SPCP_SPLICE_PWD="LiN24BcFw3CH"
export KEYTAB_PATH=$HOME
#export KEYTAB_PATH=/etc/keytabs
# Generate kerberos ticket
kinit srcpdpspcpbthpr@DEVAD.WELLPOINT.COM -k -t $KEYTAB_PATH/srcpdpspcpbthpr.keytab -V


source ~/.bash_profile

now=$(date +"%Y%m%d%H%M%S")

ENV=$1
EDGE_PATH=$2
MAPPING_TABLE_NAME=$3
CONFIG_NAME="spcp_etl_script_application_"$1".properties"

#Fetch properties from Config file -- redirect that to property file in edge node
#source that property files and get the needed params as below
echo $MAPPING_TABLE_NAME
echo $EDGE_PATH
echo $CONFIG_NAME
echo $SPLICE_CMD
echo $SPLICE_SQL_PATH

source $EDGE_PATH/$CONFIG_NAME
echo $EDGE_PATH/$CONFIG_NAME
echo $LOG_FILE_PATH

#Creating log file
log_loc=$LOG_FILE_PATH
echo "log loc== $log_loc"
SPCP_LOG_FILE=${log_loc}"/script_STATIC_LOAD_"$MAPPING_TABLE_NAME"_"$USER.log

echo "SPCP static file load wrapper triggered at $now" >>$SPCP_LOG_FILE
if [ $# -eq 3 ]
    then
        echo "Argument check completed"  >>$SPCP_LOG_FILE
    else
                echo "Error in number of arguments passed, Script needs 3 arguments for execution"  >>$SPCP_LOG_FILE
                exit 1
fi




#====================================================================================================================================================
# Run sqlshell command
sqlshell.sh -h $SPLICE_CMD -u $USER_NAME -s $SPCP_SPLICE_PWD  -f $SPLICE_SQL_PATH/${MAPPING_TABLE_NAME}.sql

#=====================================================================================================================================================

#Compare application status
if [ $? -ne 0 ]
then
echo "Static file load job for "$MAPPING_TABLE_NAME" failed.Please check the log." >>$SPCP_LOG_FILE
exit 1
else
echo "Static file load job for "$MAPPING_TABLE_NAME" completed successfully." >>$SPCP_LOG_FILE
fi



[?10/?5/?2018 4:44 PM] Solipalayam Ayyasamy, Rajkumar: 
INFO  com.anthem.hca.splice.export.fullrefresh.SpliceToRD...
INFO  com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportOperation - [SPLICE-RDBMS_CONNECTOR] Application Config Path is hdfs:///pr/hdfsapp/ve2/pdp/spcp/phi/no_gbd/r000/control/src_application_prod.conf
INFO  com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportOperation - [SPLICE-RDBMS_CONNECTOR] Query File Path is hdfs:///pr/hdfsapp/ve2/pdp/spcp/phi/no_gbd/r000/control/src_source_query.properties
INFO  com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportOperation - [SPLICE-RDBMS_CONNECTOR] The Inbound Hive schema is PDPSPCP_XM
INFO  com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportOperation - [SPLICE-RDBMS_CONNECTOR] The Audit column name is last_updt_dtm
INFO  com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportOperation - [SPLICE-RDBMS_CONNECTOR] The Audit table name is ETL_AUDIT
INFO  com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportOperation - [SPLICE-RDBMS_CONNECTOR] Splice URL is  jdbc:splice://bdpr3splice.wellpoint.com:1527/splicedb;user=srcpdpspfibthpr;password=
INFO  com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportOperation - [SPLICE-RDBMS_CONNECTOR] Splice Password alias is prod849splicepass
INFO  com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportOperation - [SPLICE-RDBMS_CONNECTOR] Splice password JCEKS locaton is  jceks://hdfs//pr/hdfsapp/ve2/pdp/spcp/phi/no_gbd/r000/bin/prod849splicemachine.jceks
ERROR com.splicemachine.client.SpliceClient - Error while granting HBase privileges, job might fail
java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.
  at com.splicemachine.db.client.am.SQLExceptionFactory40.getSQLException(SQLExceptionFactory40.java:75)
  at com.splicemachine.db.client.am.SqlException.getSQLException(SqlException.java:368)
  at com.splicemachine.db.jdbc.ClientDriver.connect(ClientDriver.java:160)
  at java.sql.DriverManager.getConnection(DriverManager.java:664)
  at java.sql.DriverManager.getConnection(DriverManager.java:270)
  at com.splicemachine.client.SpliceClient.grantHBasePrivileges(SpliceClient.java:53)
  at com.splicemachine.client.SpliceClient.setClient(SpliceClient.java:46)
  at com.splicemachine.spark.splicemachine.SplicemachineContext.<init>(SplicemachineContext.scala:94)
  at com.splicemachine.spark.splicemachine.SplicemachineContext.<init>(SplicemachineContext.scala:66)
  at com.anthem.hca.spliceexport.helper.OperationSession.<init>(OperationSession.scala:63)
  at com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportOperation.<init>(SpliceToRDBMSExportOperation.scala:26)
  at com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportDriver$.main(SpliceToRDBMSExportDriver.scala:19)
  at com.anthem.hca.splice.export.fullrefresh.SpliceToRDBMSExportDriver.main(SpliceToRDBMSExportDriver.scala)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$3.run(ApplicationMaster.scala:686) 

===============================================================
All code change is on release/rc

Step 1 - Backup all the script list file to:
spcp-etl-resources:
C:\java\git\repos\SmartPCP849Complete\spcp-etl-resources\spcp-etl-resources\scripts\backup_scripts
splice-rdbms-connector-resources
C:\java\git\repos\SmartPCP849Complete\splice-rdbms-connector-resources\splice-rdbms-connector-resources\scripts\backup_scripts

Step 2 - Empty all the .lst file in the above two folders

Step 3 - Replace the following string: (This should not be hard coded => need to be coded as part of environment variable)
@DEVAD.WELLPOINT.COM
with
@US.AD.WELLPOINT.COM 

in files:
SPCP_EXPORT_Wrapper.sh
SPCP_PRODUCT_Wrapper.sh
SPCP_MEMBER_INFO_Wrapper.sh
SPCP_VARIANCE_Wrapper.sh
SPCP_PROVIDER_PCP_GEOCODES_Wrapper.sh
SPCP_PROVIDER_INFO_Wrapper.sh
SPCP_PROVIDER_VBP_Wrapper.sh


Step 4 - commit bitbucket


git add .
git commit -m "Disable Control before data go-live and fix the production keytab as US.AD.WELLPOINT.COM"
git push origin release/rc01


Step 5 - Merged code from release/rc01 to master for the following Repository
spcp-etl-resources
splice-rdbms-connector-resources

Step 6 - Table 
C:\java\git\repos\SmartPCP849Complete\spcp-etl-resources\spcp-etl-resources\conf
spcp_etl_application_prod.properties
spcp-splice-db=pdpspcp_xm

YT,  Here is Munir's Phone No. 773-808-0767


change ticket: CTASK1090348 
Change Request number: CHG0253893
PVID: 5000849 
APID: 6676 


bdpr3r6e1pr 
srcpdpspcpbthpr

INC5898596 
INC5898597 
INC5898598 
INC5897737 


Control M job team:
Kottapalli, Sai Swaroop
Battula, Suresh

Hadoop onshore
Gupta, Vishnu
Munir
Shaikh, Sarfaroj
Khan, Asif


https://bamboo.anthem.com/browse/SPCP-SPCPEUM



Control M Job
[?10/?6/?2018 10:46 AM] Sharma, Ashish: 
SPCP_CDL_MBR_INFO_LOAD_WRAPPER_0001 - File=/bin/sh $CODE_NOGBD/scripts/spcp_member_info.lst 
SPCP_CDL_PRODUCT_TYPE_LOAD_WRAPPER_0001 -/bin/sh $CODE_NOGBD/scripts/spcp_product.lst 
SPCP_CDL_PCP_GEOCODE_LOAD_WRAPPER_0001 -/bin/sh $CODE_NOGBD/scripts/spcp_provider_pcp_geocodes.lst 
SPCP_CDL_VBP_ELGBLTY_LOAD_WRAPPER_0001 -/bin/sh $CODE_NOGBD/scripts/spcp_provider_vbp_info.lst 
For all 4 jobs files mentioned


[af35352@bdpr3r6e1pr logs]$ groups srcpdpchmobthpr
srcpdpchmobthpr : srcpdpchmobthpr bdcdh1pr
[af35352@bdpr3r6e1pr logs]$ groups srcpdpspfibthpr
srcpdpspfibthpr : srcpdpspfibthpr bdcdh1pr bdgbd



--conf 'spark.driver.extraJavaOptions=-Dsplice.spark.local.dir=/tmp/splice -Djava.security.krb5.conf=/etc/krb5.conf -Dspark.yarn.principal=${USER_NAME}@US.AD.WELLPOINT.COM -Dspark.yarn.keytab=$KEYTAB_PATH/${USER_NAME}.keytab -Dlog4j.configuration=log4j.xml  -XX:+UseCompressedOops -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=12' \
--conf 'spark.executor.extraJavaOptions=-Dsplice.spark.local.dir=/tmp/splice -Djava.security.krb5.conf=/etc/krb5.conf -Dlog4j.configuration=log4j.xml -XX:+UseCompressedOops -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=12'  \



--conf 'spark.driver.extraJavaOptions=-Djava.security.krb5.conf=/etc/krb5.conf -Dspark.yarn.principal=srcpdpspcpbthdv@DEVAD.WELLPOINT.COM -Dspark.yarn.keytab=/dv/app/ve2/pdp/spcp/phi/no_gbd/r000/bin/srcpdpspcpbthdv.keytab -Dlog4j.configuration=log4j.xml  -XX:+UseCompressedOops -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=12' \ 

hadoop fs -getfac1 -R -m group:srcpdpspcpbthpr:rwx /tmp
hadoop fs -setfacl -R -m group:srcpdpspcpbthpr:rwx /pr/data/ve2/pdp/spcp/phi/gbd/r000/outbound
hadoop fs -setfacl -R -m default:group:srcpdpspcpbthpr:rwx /pr/data/ve2/pdp/spcp/phi/gbd/r000/outbound 

setfacl -R -m group:srcpdpspcpbthpr:rwx /pr/data/ve2/pdp/spcp/phi/gbd/r000
setfacl -R -m default:group:srcpdpspcpbthpr:rwx /pr/data/ve2/pdp/spcp/phi/gbd/r000 



Us
+ purging + date as part 


Big Data CDH Support = SNOW ticket team for Hadoop Admin

BI PROD APP SUPPORT 1

Here are the Production apigee details which needs be used for validations.

Token URL: https://api.anthem.com/v1/oauth/accesstoken
Client id: 33bbd32611db4f6990ed0b9e7867a09d
Secret:  f4c1f24719e6173008af2e97971fec61e9cfbd39612c8b3e67443a116429bf3c
Apikey: raLPQxTXhdFosKq1z3UbYQuFVZS5cbGP

Service apigee URLs

https://api.anthem.com/v1/spcp/members/pcp/selection/smartSelection


================================


[?10/?9/?2018 8:44 AM] Chunduri, Sitaram: 
: java.lang.IllegalStateException: LifecycleProcessor not initialized - call 'refresh' before invoking lifecycle methods via the context: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@7f2ca6f8: startup date [Tue Oct 09 10:42:24 EDT 2018]; parent: org.springframework.context.annotation.AnnotationConfigApplicationContext@208f0007  
[?10/?9/?2018 8:49 AM] Chunduri, Sitaram: 
2018-10-09 10:42:39.228 [] [main] INFO o.s.aop.framework.CglibAopProxy -Final method [public final java.lang.Object org.springframework.retry.support.RetryTemplate.execute(org.springframework.retry.RetryCallback,org.springframework.retry.RetryState) throws java.lang.Throwable,org.springframework.retry.ExhaustedRetryException] cannot get proxied via CGLIB: Calls to this method will NOT be routed to the target instance and might lead to NPEs against uninitialized fields in the proxy instance.  
2018-10-09 10:42:39.228 [] [main] WARN o.s.aop.framework.CglibAopProxy -Unable to proxy interface-implementing method [public final java.lang.Object org.springframework.retry.support.RetryTemplate.execute(org.springframework.retry.RetryCallback,org.springframework.retry.RetryState) throws java.lang.Throwable,org.springframework.retry.ExhaustedRetryException] because it is marked as final: Consider using interface-based JDK proxies instead! 
[?10/?9/?2018 8:50 AM] Chunduri, Sitaram: 

   line: 2018-10-09 10:42:38.594 [] [main] INFO c.n.c.sources.URLConfigurationSource -To enable URLs as dynamic configuration sources, define System property archaius.configurationSource.additionalUrls or make config.properties available on classpath.  
[?10/?9/?2018 8:50 AM] Chunduri, Sitaram: 

   line: 2018-10-09 10:42:38.594 [] [main] INFO c.n.c.sources.URLConfigurationSource -To enable URLs as dynamic configuration sources, define System property archaius.configurationSource.additionalUrls or make config.properties available on classpath.  


PROD user : p_ant_wlp_vbpeligsrc	Password: D5p@zDagBZ

/pr/data/ve2/pdp/spcp/phi/no_gbd/r000/outbound
/pr/data/ve2/pdp/spcp/phi/gbd/r000/outbound


https://efx.anthem.com/myfilegateway/home.do#login 


 sftp p_ant_wlp_vbpeligsrc@efx.anthem.com
 *******
 lcd /pr/data/ve2/pdp/spcp/phi/gbd/r000/outbound
 ls
 put vbp_eligibility.tsv
 
sftp p_ant_wlp_vbpeligsrc@efx.anthem.com <<EOF
lcd /pr/data/ve2/pdp/spcp/phi/gbd/r000/outbound
put vbp_eligibility.tsv
EOF

Hi Munir
We are working on a production issue with the SFTP team, might need you 2 minutes
please let us know if you can help us the following:

Step 1 : login as srcpdpspcpbthpr  to prod 3
Step 2: sftp p_ant_wlp_vbpeligsrc@efx.anthem.com  

If you got no password prompt and go straight to the sftp> prompt, we are good
that means our id has the keyless sftp setup already
Do you mind to help us to do a quick test?


 
 URL PROD	https://efx.anthem.com/myfilegateway/home.do#login 
 
 PROD user : p_ant_wlp_vbpeligsrc	D5p@zDagBZ
 URL Test	https://efx.uat.va.anthem.com/myfilegateway
 
 Test User: t_ant_wlp_vbpeligsrc	W5g@42U8VS
 ----> this is not setup yet
 sftp t_ant_wlp_vbpeligsrc@efx.uat.va.anthem.com
 *******
 lcd /pr/data/ve2/pdp/spcp/phi/gbd/r000/outbound
 ls
 put vbp_eligibility.tsv
 
 
 su srcpdpspcpbthts
 ;password=LiN24BcFw3CH
 
 W5g@42U8VS
 
 
unix2dos /pr/data/ve2/pdp/spcp/phi/gbd/r000/outbound/vbp_eligibility.tsv /pr/data/ve2/pdp/spcp/phi/gbd/r000/outbound/vbp_eligibility.tsv
sftp t_ant_wlp_vbpeligsrc@efx.anthem.com <<EOF
lcd /ts/data/ve2/pdp/spcp/phi/gbd/r000/outbound
put vbp_eligibility.tsv
EOF 
 
unix2dos /ts/data/ve2/pdp/spcp/phi/gbd/r000/outbound/vbp_eligibility.tsv 
sftp t_ant_wlp_vbpeligsrc@efx.anthem.com <<EOF
lcd /ts/data/ve2/pdp/spcp/phi/gbd/r000/outbound
put vbp_eligibility.tsv
EOF
 
 public key for Prod 3 Server
 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDN6A4b0vMuPFUP8wNFAEWroiDdADwHo38ixA7QI4Y4iqB8gjpKDm0vAlKDsRQeWaiKuq5auq8+ocVd5fgj7DbyExahElrUP0ExBLGhJDtLnNQAIk405227mUOjVZfDFbet+U4EXD959coLE3vGC2EGrLP0cw60GCQhoM3JBeDx+U9ojixlrXso0+JG2tYLyFh0Q2mYpfOEwXZ/8OPcHf9CvhlSRm/yTX5/tVl96IG3PjudmbmGPQu5FKuiW4/SP/j7rmzNoS+s8e44ndgTy9vft5tzpYw0s7LTCnB6oCM+FfhppTgLzNftfdNNQKdNPOmfNLQfgVZYEqmpWu5WmXI5 srcpdpspcpbthpr@bdpr3r6e1pr.wellpoint.com 
 
 
 public key for SIT edgenode
 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCvVu8XaXGpV3iKOrFvXI93aSPIH9vTDAPjAvTy7DWl4oflispvWDpZFsySuy2w4p4lunrtDppAAc6ZyeemjS/JiZGRHUoGN57oTrYERfEjwwU6QPoCoolo9sXiNt1NvctQCkJ+YvQcgg816ut5Wo2sh8Cq5EMWGgz8IfUElmqbSFf2UH4erAU5U1srs1X94rB2PbWNapsi1GS6OTk1cipu5qBz0DTv/1y9U1SBbjDnGGPCgrLZhvk7Zd6+RLLhqwmuD4qWO3kHPNPTTlthCZrR6bCHQKb8gln8GKV26sEpbYAgG6GQuh8mU4pAPTNf/5sEGQVZ+AcB01hM1odyV+0V srcpdpspcpbthts@dwbdtest1r1e.wellpoint.com 
 
 
 PRod CDL folder landing folder for WGS files
 
 
 
 
 Test edgenode Path: 	/ts/data/ve2/cdl/wgsp/phi/gbd/r000/inbound 
 Prod edgenode Path:	/pr/data/ve2/cdl/wgsp/phi/gbd/r000/inbound 
 
 CDL team will red the files from this location and load into Splice tables. After the table loads are successful, they will be move to following Archive:
 
 Archive edge node Test	/ts/hdfsdata/ve2/cdl/wgsp/phi/gbd/r000/archive/
 Archive edge node Prod	/pr/hdfsdata/ve2/cdl/wgsp/phi/gbd/r000/archive/



Splunk report for transactions:
Change index=apigee_prod for production
-----------------------------------------------------------
index=apigee_test "/v1/spcp/members/pcp/select/smartSelection" | eval TotalTime = TransactionSentEndtime - TransactionReceivedStartTime
| eval TargetTime = ResponseReceivedStartTime - RequestSentEndTime
| eval RequestNetTime = TransactionReceivedEndtime - TransactionReceivedStartTime 
| eval TargetNetTime = RequestSentEndTime - RequestSentStartTime 
| eval ResponseRecvNetTime = ResponseReceivedEndTime - ResponseReceivedStartTime
| eval ResponseSentNetTime = TransactionSentEndtime - TransationSentStartTime
| eval ProxyTime = TotalTime - TargetTime - RequestNetTime - TargetNetTime - ResponseSentNetTime - ResponseRecvNetTime
| eval ProxyReqTime = RequestSentStartTime - TransactionReceivedEndtime
| eval ProxyRespTime = TransationSentStartTime - ResponseReceivedEndTime
| eval RequestSentStart_time=strftime(RequestSentStartTime/1000, "%m-%d-%Y %H:%M:%S.%3N") 
| eval RequestSentEnd_time=strftime(RequestSentEndTime/1000, "%m-%d-%Y %H:%M:%S.%3N")
| eval ResponseReceivedStart_time=strftime(RequestSentEndTime/1000, "%m-%d-%Y %H:%M:%S.%3N") 
| eval formatted_time=strftime(TransactionReceivedStartTime/1000, "%m-%d-%Y %H:%M:%S")
| table Sender formatted_time RequestSentStart_time RequestSentEnd_time ResponseReceivedStart_time URI TotalTime ProxyTime TargetTime RequestNetTime TargetNetTime ResponseRecvNetTime ResponseSentNetTime ProxyReqTime ProxyRespTime
-----------------------------------------


AB view
CREATE  VIEW [TS_PDPSPCP_XM].[MBR_INFO] 
AS
SELECT 'MBR_INFO1' TAB_NM, A.* FROM TS_PDPSPCP_XM.MBR_INFO1 A, TS_PDPSPCP_XM.SPCP_CNTRL B WHERE B.STATUS = 'Y' AND TABLE_NM = 'MBR_INFO1'
UNION ALL
SELECT 'MBR_INFO2' TAB_NM, A.* FROM TS_PDPSPCP_XM.MBR_INFO2 A, TS_PDPSPCP_XM.SPCP_CNTRL B WHERE B.STATUS = 'Y' AND TABLE_NM = 'MBR_INFO2' 


Release management leader:
Michael Mcdonald

Release management on Deloitte side:
Shah, Chintan

OTHER MEDICAL PRODUCTS
12345678901234567890



HC_ID			12	VARCHAR	20
PROD_FMLY_TYPE_CD	12	VARCHAR	20
MBU_CF_CD		12	VARCHAR	20
PCP_ID			12	VARCHAR	20

Target RPT_MBR_PCP_DTL
Sselect count(*) from PDPSPCP.RPT_MBR_PCP_DTLS


48198437

Source table:
select count(*) from cdledwd_allphi.MBR_PCP where trim(rcrd_stts_cd) <> 'DEL' and trim(mbrshp_sor_cd) in ('808','815')

43604224

https://bdpr3r6mn3pr.wellpoint.com:8090/proxy/application_1540739177921_4057/SQL/execution?id=1

XM view cannot 

Emergency ticket process
