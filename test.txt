==============================================================================
***** Manual Testing on HBase Connection From Spark ******************
=================================================================================
Step 1 - Start the Spark Shell
spark2-shell \
--principal AF35352 \
--keytab /home/af35352/keytabstore/AF35352.keytab  \
--jars hdfs:///dv/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/bin/af35352Test/ds-cogx-padp-etl-2.0.5.jar 

Step 2 - Create a simple DataFrame:
import java.io.File
import java.util.UUID

import org.apache.hadoop.fs.FileSystem
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.{DataFrame, SQLContext}
import org.apache.spark.sql.functions.{current_timestamp, lit}

sc.hadoopConfiguration.set("parquet.enable.summary-metadata", "false")
val sqlContext = new SQLContext(sc)

:paste

var uuid: String = null;
var newData1 = List(("",""));
for (i<-1L to 5L)
{
  uuid = UUID.randomUUID().toString()
  newData1 = newData1:::List((uuid,"value" + uuid))
}
newData1=newData1.drop(1)
val df3 = sqlContext.createDataFrame(sc.parallelize(newData1, 5).map(x => (x._1,x._2))).toDF("rowKey", "jsonData")
df3.show()
*** Control D *****


val columnFamily = "um"
val columnName = "jsonData"


import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.HBaseAdmin
:paste

    val putRDD = df3.rdd.map(x => {
          val rowKey = x.getAs[String]("rowKey")
          val holder = x.getAs[String]("jsonData")
          print(rowKey, holder)
          val p = new Put(Bytes.toBytes(rowKey))
          p.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(columnName), Bytes.toBytes(holder))
          (new org.apache.hadoop.hbase.io.ImmutableBytesWritable, p)
        }
    )

*********** control D ***************

import com.google.gson.GsonBuilder
import org.apache.avro.io.EncoderFactory
import org.apache.avro.reflect.ReflectData
import org.apache.avro.reflect.ReflectDatumWriter
import java.io.ByteArrayOutputStream
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.mapreduce.{Job, JobStatus}
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.{TableMapReduceUtil, TableOutputFormat}
import org.apache.spark.rdd.PairRDDFunctions
import org.apache.hadoop.fs.{FSDataInputStream, Path}
val hdfs: FileSystem = FileSystem.get(spark.sparkContext.hadoopConfiguration)


:paste

  def getMapReduceJobConfiguration():Configuration = {
      val job = Job.getInstance(HBaseConfiguration.create(), "HDFS-to-HBase ETL")
      job.setOutputFormatClass(new org.apache.hadoop.hbase.mapreduce.TableOutputFormat[ImmutableBytesWritable].getClass)
     var confIT = job.getConfiguration().iterator()
     println ("*********************************************************")
     while (confIT.hasNext())
     {
       var it = confIT.next()
       if (it.getKey.toString().contains("hbase"))
       {
       
       println("Key: "+ it.getKey())
       println("Value: " + it.getValue())
       }
     }
     println ("*********************************************************")
     
      job.getConfiguration().addResource( (hdfs.open(new Path("hdfs:///dv/hdfsapp/ve2/ccp/cogx/phi/gbd/r000/control/af35352Test/hbase-site.xml")) ) )    
      //job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE,tableName)
      job.getConfiguration
  }
  
val config = getMapReduceJobConfiguration()
config.set(TableOutputFormat.OUTPUT_TABLE,"dv_hb_ccpcogx_gbd_r000_in:um_auth")
HBaseAdmin.checkHBaseAvailable(config)
  
new PairRDDFunctions(putRDD).saveAsNewAPIHadoopDataset(config)

=============================================================
